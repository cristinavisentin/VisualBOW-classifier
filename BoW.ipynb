{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2 (Bag-of-words classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project is the implementation of an image classifier based on the bag-of-words approach. \n",
    "\n",
    "The provided dataset, in `images` folder, contains 15 categories (office, kitchen, living room, bedroom, store, industrial, tall building, inside city, street, highway, coast, open country, mountain, forest, suburb) and is already divided in training set and test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics.pairwise import pairwise_kernels, chi2_kernel\n",
    "from sklearn.multiclass import OutputCodeClassifier\n",
    "\n",
    "from utility.visualVocabulary import VisualVocabulary\n",
    "from utility.histograms import *\n",
    "from utility.images import load_images\n",
    "from utility.sift import extract_sift_descriptors\n",
    "from utility.function_utils import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the size of the visual vocabulary and the number of SIFT descriptors choosen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLUSTER_NUMBER = 200\n",
    "N_SIFT_FEATURES = 800\n",
    "N_SUBSET_SIFT = 200000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load train and test images from folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = load_images('images/train')\n",
    "test_images = load_images(\"images/test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here we create the vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You don't need to execute this cell because the vocabulary was already constructed.\n",
    "\n",
    "It is saved as picke file in the folder `pickled`.\n",
    "\n",
    "We decide to keep these line code to show you how we computed the different values that we will use soon.\n",
    "\n",
    "WARNING: unfortunately, due to the computational heavy of the UNC, we implement a code which use the GPU to calculate these values. So to run the code below you must have a GPU with CUDA's drivers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vv = VisualVocabulary(CLUSTER_NUMBER, N_SIFT_FEATURES, N_SUBSET_SIFT)\n",
    "vocabulary, quantized_train_images, UNC_train_images, pyramid_train_histogram = vv.create_vocabulary(train_images)\n",
    "vv_file = open('pickled/vocabulary.pkl', 'wb')\n",
    "pickle.dump(vv, vv_file)\n",
    "vv_file = open('pickled/quantized.pkl', 'wb')\n",
    "pickle.dump(quantized_train_images, vv_file)\n",
    "vv_file = open('pickled/UNC.pkl', 'wb')\n",
    "pickle.dump(UNC_train_images, vv_file)\n",
    "vv_file = open('pickled/pyramid.pkl', 'wb')\n",
    "pickle.dump(pyramid_train_histogram, vv_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this cell to load the vocabulary from the pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vv_file = open('pickled/vocabulary.pkl', 'rb')\n",
    "vv = pickle.load(vv_file)\n",
    "vv_file = open('pickled/quantized.pkl', 'rb')\n",
    "quantized_train_images = pickle.load(vv_file)\n",
    "vv_file = open('pickled/UNC.pkl', 'rb')\n",
    "UNC_train_images = pickle.load(vv_file)\n",
    "vv_file = open('pickled/pyramid.pkl', 'rb')\n",
    "pyramid_train_histogram = pickle.load(vv_file)\n",
    "vv_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtain train histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_histograms = create_histograms(quantized_train_images, vv.vocabulary)\n",
    "UNC_train_histogram = create_UNC_histogram(UNC_train_images, vv.vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to plot one histogram for every class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(5, 3, figsize=(30, 50))\n",
    "indices = [0, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400]\n",
    "i = [train_histograms[idx]['histogram'] for idx in indices]\n",
    "l = [train_histograms[idx]['label'] for idx in indices]\n",
    "ax = np.ravel(ax)\n",
    "for index, hist in enumerate(i):\n",
    "    ax[index].bar(list(range(CLUSTER_NUMBER)), hist)\n",
    "    ax[index].set_xlabel('Visual word')\n",
    "    ax[index].set_ylabel('Frequency')\n",
    "    ax[index].set_title(l[index])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histograms normalization through TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf_train = calculate_idf(train_histograms, len(vv.vocabulary))\n",
    "normalized_train_histograms = tfidf_histograms(train_histograms, idf_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And plot the resulting ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(5, 3, figsize=(30, 50))\n",
    "i = [normalized_train_histograms[idx]['tf_idf'] for idx in indices]\n",
    "l = [normalized_train_histograms[idx]['label'] for idx in indices]\n",
    "ax = np.ravel(ax)\n",
    "for index, hist in enumerate(i):\n",
    "    ax[index].bar(list(range(CLUSTER_NUMBER)), hist)\n",
    "    ax[index].set_xlabel('Visual word')\n",
    "    ax[index].set_ylabel('Frequency')\n",
    "    ax[index].set_title(l[index])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test and accessment phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute test histograms and normalize them through TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_histograms(images):\n",
    "    test_sift_descriptors = extract_sift_descriptors(images, N_SIFT_FEATURES)\n",
    "    quantized_test_images = vv.quantize_images(test_sift_descriptors)\n",
    "    test_histograms = create_histograms(quantized_test_images, vv.vocabulary)\n",
    "    UNC_test_images = vv.UNC_images(test_sift_descriptors)\n",
    "    UNC_test_histograms = create_UNC_histogram(UNC_test_images, vocabulary)\n",
    "    pyramid_test_histogram = vv.compute_weighted_pyramid_descriptor(images, 2)\n",
    "    return test_histograms, UNC_test_histograms, pyramid_test_histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decide to keep these line code to show you how we computed the different values that we will use soon.\n",
    "\n",
    "WARNING: unfortunately, due to the heavy computation of the UNC, we implement a code which use the GPU to calculate these values. So to run the code below you must have a GPU with CUDA's drivers.\n",
    "\n",
    "Use the values that we have already computed for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_histograms, UNC_test_histogram, pyramid_test_histogram = get_test_histograms(test_images)\n",
    "vv_file = open('pickled/quantized_test.pkl', 'wb')\n",
    "pickle.dump(test_histograms, vv_file)\n",
    "vv_file = open('pickled/UNC_test.pkl', 'wb')\n",
    "pickle.dump(UNC_test_histogram, vv_file)\n",
    "vv_file = open('pickled/pyramid_test.pkl', 'wb')\n",
    "pickle.dump(pyramid_test_histogram , vv_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run all the other cells from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vv_file = open('pickled/quantized_test.pkl', 'rb')\n",
    "test_histograms = pickle.load(vv_file)\n",
    "vv_file = open('pickled/UNC_test.pkl', 'rb')\n",
    "UNC_test_histogram = pickle.load(vv_file)\n",
    "vv_file = open('pickled/pyramid_test.pkl', 'rb')\n",
    "pyramid_test_histogram = pickle.load(vv_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly we need to normalize with the IDF computed on the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_test_histograms = tfidf_histograms(test_histograms, idf_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we create the effective train and test sets, suitable for input into the classification models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [item['tf_idf'] for item in normalized_train_histograms]\n",
    "y_train = [item['label'] for item in normalized_train_histograms]\n",
    "X_test = [item['tf_idf'] for item in normalized_test_histograms]\n",
    "y_test = [item['label'] for item in normalized_test_histograms]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn(X_train, y_train, X_test, y_test, n_neighbors=10, metric=chi_squared_distance):\n",
    "    knn = KNeighborsClassifier(n_neighbors=n_neighbors, metric=metric).fit(X_train, y_train)\n",
    "    y_pred = knn.predict(X_test)\n",
    "    print(\"Test accuracy = \", accuracy_score(y_test, y_pred) * 100)\n",
    "    show_confusion_matrix(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    print(\"\\nClassification Report:\\n\")\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results with the Cosine Similarity distance metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = knn(X_train, y_train, X_test, y_test, metric='cosine')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results with distance metric \"Chi-squared\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = knn(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear multiclass SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_svm = OneVsRestClassifier(SVC(kernel='linear')).fit(X_train, y_train)\n",
    "y_pred = linear_svm.predict(X_test)\n",
    "print(\"Test accuracy = \", accuracy_score(y_test, y_pred) * 100)\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(report)\n",
    "show_confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian multiclass SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_svm = OneVsRestClassifier(SVC(kernel='rbf')).fit(X_train, y_train)\n",
    "y_pred = gaussian_svm.predict(X_test)\n",
    "print(\"Test accuracy = \", accuracy_score(y_test, y_pred) * 100)\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(report)\n",
    "show_confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM with generalized Gaussian kernel based on the χ2 distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 3\n",
    "K_train = chi2_kernel(X_train, X_train, gamma=GAMMA)\n",
    "K_test = chi2_kernel(X_test, X_train, gamma=GAMMA)\n",
    "\n",
    "chi_svm = OneVsRestClassifier(SVC(kernel='precomputed'))\n",
    "clf_chi_svm= chi_svm.fit(K_train, y_train)\n",
    "y_pred = clf_chi_svm.predict(K_test)\n",
    "\n",
    "print(\"Test accuracy = \", accuracy_score(y_test, y_pred) * 100)\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(report)\n",
    "show_confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiclass SVM using the Error Correcting Output Code approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecoc_chi = OutputCodeClassifier(estimator=chi_svm, code_size=45, random_state=42).fit(K_train, y_train)\n",
    "\n",
    "y_pred_ecoc_chi = ecoc_chi.predict(K_test)\n",
    "print(\"Test accuracy = \", accuracy_score(y_test, y_pred_ecoc_chi) * 100)\n",
    "report = classification_report(y_test, y_pred_ecoc_chi)\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(report)\n",
    "\n",
    "show_confusion_matrix(y_test, y_pred_ecoc_chi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiclass SVM using the Codeword Uncertainty formula to perform a Soft Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_UNC = [item['histogram'] for item in UNC_train_histogram]\n",
    "y_train_UNC = [item['label'] for item in UNC_train_histogram]\n",
    "X_test_UNC = [item['histogram'] for item in UNC_test_histogram]\n",
    "y_test_UNC = [item['label'] for item in UNC_test_histogram]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 3\n",
    "K_train_UNC = chi2_kernel(X_train_UNC, X_train_UNC, gamma=GAMMA)\n",
    "K_test_UNC = chi2_kernel(X_test_UNC, X_train_UNC, gamma=GAMMA)\n",
    "\n",
    "UNC_svm = OneVsRestClassifier(SVC(kernel='precomputed')).fit(K_train_UNC, y_train_UNC)\n",
    "\n",
    "y_pred_UNC = UNC_svm.predict(K_test_UNC)\n",
    "print(\"Test accuracy = \", accuracy_score(y_test_UNC, y_pred_UNC) * 100)\n",
    "\n",
    "report = classification_report(y_test_UNC, y_pred_UNC)\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(report)\n",
    "\n",
    "show_confusion_matrix(y_test_UNC, y_pred_UNC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiclass SVM using the the Spatial Pyramid Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pyramid = [item['histogram'] for item in pyramid_train_histogram]\n",
    "y_train_pyramid = [item['label'] for item in pyramid_train_histogram]\n",
    "X_test_pyramid = [item['histogram'] for item in pyramid_test_histogram]\n",
    "y_test_pyramid = [item['label'] for item in pyramid_test_histogram]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "levels = 2 \n",
    "K_train, K_test = compute_kernel_matrix(X_train_pyramid, X_test_pyramid, levels)\n",
    "\n",
    "pyramid_svm = OneVsRestClassifier(SVC(kernel='precomputed')).fit(K_train, y_train_pyramid)\n",
    "y_pred_pyramid = pyramid_svm.predict(K_test)\n",
    "\n",
    "print(\"Test accuracy = \", accuracy_score(y_test_pyramid, y_pred_pyramid) * 100)\n",
    "\n",
    "report = classification_report(y_test_pyramid, y_pred_pyramid)\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(report)\n",
    "\n",
    "show_confusion_matrix(y_test_pyramid, y_pred_pyramid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
